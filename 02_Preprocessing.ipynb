{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b1abc20-e93c-4f3c-8212-8651103cac13",
   "metadata": {},
   "source": [
    "# PREPROCESSING\n",
    "\n",
    "- [Importar librerías](#Importar-librerías)\n",
    "- [Lectura de los datasets](#Lectura-de-los-datasets)\n",
    "- [Eliminación de características irrelevantes](#Eliminación-de-características-irrelevantes)\n",
    "- [Manejo de datos faltantes](#Manejo-de-datos-faltantes)\n",
    "- [Manejo de outliers](#Manejo-de-outliers)\n",
    "- [Gestión de tipos](Gestión-de-tipos)\n",
    "- [Codificación-de-variables-categóricas](Codificación-de-variables-categóricas)\n",
    "- [Normalización y estandarización](#Normalización-y-estandarización)\n",
    "- [Transformaciones de datos](#Transformaciones-de-datos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479f4108-3806-4fd7-887f-2cbb60742249",
   "metadata": {},
   "source": [
    "## Importar librerías"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569c278a-3eee-4173-9cec-4d4aa9f0f288",
   "metadata": {},
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e229e11-fbf9-4ea7-872e-18e336e62b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring tensorflow-macos: markers 'platform_system == \"Darwin\"' don't match your environment\n",
      "Ignoring tensorflow-metal: markers 'platform_system == \"Darwin\"' don't match your environment\n",
      "Requirement already satisfied: kaggle==1.5.12 in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from -r requirements.txt (line 2)) (1.5.12)\n",
      "Requirement already satisfied: plotly==6.0.0 in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from -r requirements.txt (line 3)) (6.0.0)\n",
      "Requirement already satisfied: scipy==1.15.2 in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from -r requirements.txt (line 4)) (1.15.2)\n",
      "Requirement already satisfied: numpy>=1.26.4 in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from -r requirements.txt (line 5)) (2.0.2)\n",
      "Requirement already satisfied: pandas>=1.5.0 in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from -r requirements.txt (line 6)) (2.2.3)\n",
      "Requirement already satisfied: matplotlib>=3.5.0 in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from -r requirements.txt (line 7)) (3.10.0)\n",
      "Requirement already satisfied: seaborn>=0.12.0 in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from -r requirements.txt (line 8)) (0.13.2)\n",
      "Requirement already satisfied: tensorflow>=2.12.0 in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from -r requirements.txt (line 11)) (2.18.0)\n",
      "Requirement already satisfied: scikit-learn>=1.2.0 in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from -r requirements.txt (line 14)) (1.6.1)\n",
      "Requirement already satisfied: scikeras>=0.11.0 in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from -r requirements.txt (line 15)) (0.13.0)\n",
      "Requirement already satisfied: keras>=2.16.0 in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from -r requirements.txt (line 16)) (3.8.0)\n",
      "Requirement already satisfied: protobuf>=3.19.0 in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from -r requirements.txt (line 19)) (5.29.3)\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from kaggle==1.5.12->-r requirements.txt (line 2)) (1.17.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from kaggle==1.5.12->-r requirements.txt (line 2)) (2024.12.14)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from kaggle==1.5.12->-r requirements.txt (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: requests in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from kaggle==1.5.12->-r requirements.txt (line 2)) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from kaggle==1.5.12->-r requirements.txt (line 2)) (4.67.1)\n",
      "Requirement already satisfied: python-slugify in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from kaggle==1.5.12->-r requirements.txt (line 2)) (8.0.4)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from kaggle==1.5.12->-r requirements.txt (line 2)) (2.3.0)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from plotly==6.0.0->-r requirements.txt (line 3)) (1.28.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from plotly==6.0.0->-r requirements.txt (line 3)) (24.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from pandas>=1.5.0->-r requirements.txt (line 6)) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from pandas>=1.5.0->-r requirements.txt (line 6)) (2025.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from matplotlib>=3.5.0->-r requirements.txt (line 7)) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from matplotlib>=3.5.0->-r requirements.txt (line 7)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from matplotlib>=3.5.0->-r requirements.txt (line 7)) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from matplotlib>=3.5.0->-r requirements.txt (line 7)) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from matplotlib>=3.5.0->-r requirements.txt (line 7)) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from matplotlib>=3.5.0->-r requirements.txt (line 7)) (3.2.1)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from tensorflow>=2.12.0->-r requirements.txt (line 11)) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow>=2.12.0->-r requirements.txt (line 11)) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow>=2.12.0->-r requirements.txt (line 11)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow>=2.12.0->-r requirements.txt (line 11)) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow>=2.12.0->-r requirements.txt (line 11)) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow>=2.12.0->-r requirements.txt (line 11)) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow>=2.12.0->-r requirements.txt (line 11)) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow>=2.12.0->-r requirements.txt (line 11)) (3.4.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow>=2.12.0->-r requirements.txt (line 11)) (75.8.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow>=2.12.0->-r requirements.txt (line 11)) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow>=2.12.0->-r requirements.txt (line 11)) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow>=2.12.0->-r requirements.txt (line 11)) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow>=2.12.0->-r requirements.txt (line 11)) (1.70.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow>=2.12.0->-r requirements.txt (line 11)) (2.18.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow>=2.12.0->-r requirements.txt (line 11)) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow>=2.12.0->-r requirements.txt (line 11)) (0.4.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from scikit-learn>=1.2.0->-r requirements.txt (line 14)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from scikit-learn>=1.2.0->-r requirements.txt (line 14)) (3.5.0)\n",
      "Requirement already satisfied: rich in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from keras>=2.16.0->-r requirements.txt (line 16)) (13.9.4)\n",
      "Requirement already satisfied: namex in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from keras>=2.16.0->-r requirements.txt (line 16)) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from keras>=2.16.0->-r requirements.txt (line 16)) (0.14.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from requests->kaggle==1.5.12->-r requirements.txt (line 2)) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from requests->kaggle==1.5.12->-r requirements.txt (line 2)) (3.10)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from python-slugify->kaggle==1.5.12->-r requirements.txt (line 2)) (1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from rich->keras>=2.16.0->-r requirements.txt (line 16)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from rich->keras>=2.16.0->-r requirements.txt (line 16)) (2.19.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from tqdm->kaggle==1.5.12->-r requirements.txt (line 2)) (0.4.6)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow>=2.12.0->-r requirements.txt (line 11)) (0.45.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=2.16.0->-r requirements.txt (line 16)) (0.1.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow>=2.12.0->-r requirements.txt (line 11)) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow>=2.12.0->-r requirements.txt (line 11)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow>=2.12.0->-r requirements.txt (line 11)) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\lucia\\miniconda3\\envs\\proyecto_ml\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow>=2.12.0->-r requirements.txt (line 11)) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt\n",
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271ae49d-2beb-4f4a-bf54-e7dd28924921",
   "metadata": {},
   "source": [
    "## Lectura de los datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4f6099f-ec82-4e4a-b206-552e65792c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: (891, 12)\n",
      "Test dataset: (418, 11)\n"
     ]
    }
   ],
   "source": [
    "INPUT_ZIP = \"./00_Data/Raw/archive.zip\"  # Directorio del zip\n",
    "OUTPUT_FOLDER = \"./00_Data/Raw\"  # Directorio de destino\n",
    "TRAIN_FILENAME = \"train.csv\"  # Nombre del fichero de entrenamiento\n",
    "TEST_FILENAME = \"test.csv\"  # Nombre del fichero de entrenamiento\n",
    "\n",
    "def fetch_data(input_path=INPUT_ZIP, output_dir=OUTPUT_FOLDER):\n",
    "    \"\"\"\n",
    "    Extrae el contenido de un archivo ZIP en un directorio de destino.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "    input_path : str, opcional\n",
    "        Ruta al archivo ZIP que se desea descomprimir. El valor predeterminado es la variable 'INPUT_ZIP'.\n",
    "        \n",
    "    output_dir : str, opcional\n",
    "        Directorio en el cual se extraerá el contenido del archivo ZIP. Si el directorio no existe,\n",
    "        será creado automáticamente. El valor predeterminado es la variable 'OUTPUT_FOLDER'.\n",
    "\n",
    "    Comportamiento:\n",
    "    ---------------\n",
    "    - Crea el directorio de destino si no existe.\n",
    "    - Descomprime el archivo ZIP en el directorio de destino.\n",
    "\n",
    "    Excepciones:\n",
    "    ------------\n",
    "    Puede lanzar una excepción si el archivo ZIP no existe o si hay problemas al descomprimirlo.\n",
    "\n",
    "    Ejemplo de uso:\n",
    "    ---------------\n",
    "    fetch_data('data.zip', 'output/')\n",
    "    \"\"\"\n",
    "    # Comprobación de que el directorio de destino existe\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Descomprime el archivo ZIP en caso de que no haya ningún csv en la carpeta\n",
    "    if(len([file for file in os.listdir(output_dir) if file.endswith('.csv')]) == 0):\n",
    "        with zipfile.ZipFile(input_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(output_dir)\n",
    "\n",
    "\n",
    "def load_data(directory=OUTPUT_FOLDER, filename=TRAIN_FILENAME):\n",
    "    \"\"\"\n",
    "    Lee un archivo CSV desde el directorio especificado.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "    directory : str\n",
    "        El directorio donde se encuentra el archivo CSV.\n",
    "        \n",
    "    filename : str\n",
    "        El nombre del archivo CSV a leer (incluyendo la extensión .csv).\n",
    "\n",
    "    Retorna:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Un DataFrame de pandas que contiene los datos del archivo CSV.\n",
    "\n",
    "    Excepciones:\n",
    "    ------------\n",
    "    FileNotFoundError:\n",
    "        Se lanza si el archivo no existe en el directorio dado.\n",
    "    \n",
    "    Ejemplo de uso:\n",
    "    ---------------\n",
    "    df = read_csv_from_directory('data', 'file.csv')\n",
    "    \"\"\"\n",
    "    # Construir la ruta completa al archivo CSV\n",
    "    file_path = os.path.join(directory, filename)\n",
    "\n",
    "    # Verificar si el archivo existe\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"El archivo {filename} no se encuentra en el directorio {directory}\")\n",
    "\n",
    "    # Leer el archivo CSV en un DataFrame\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "fetch_data()\n",
    "df_train = load_data(OUTPUT_FOLDER, TRAIN_FILENAME)\n",
    "df_test = load_data(OUTPUT_FOLDER, TEST_FILENAME)\n",
    "\n",
    "print(\"Train dataset:\", df_train.shape)\n",
    "print(\"Test dataset:\", df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31acf507-a923-4ca0-ac3c-c5937618f880",
   "metadata": {},
   "source": [
    "Como vemos, en el dataset de test no se incluyen las 12 columnas (quitan la de predicción)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222e17fb-b951-46cc-b517-63bdb19041b5",
   "metadata": {},
   "source": [
    "## Eliminación de características irrelevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7734716-504a-4a91-b00f-8d010ebeadec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionamos las columnas con las que nos queremos quedar\n",
    "cols_keep = [\"Sex\", \"Pclass\", \"Age\", \"SibSp\", \"Parch\", \"Embarked\", \"Cabin\", \"Fare\"]\n",
    "predict_col = \"Survived\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2083f27e-09c2-4fdb-a007-9baab6bf2b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las columnas originales son: ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n",
      "Las columnas tras eliminar irrelevantes son: ['Sex', 'Pclass', 'Age', 'SibSp', 'Parch', 'Embarked', 'Cabin', 'Fare', 'Survived']\n"
     ]
    }
   ],
   "source": [
    "print(\"Las columnas originales son:\", df_train.columns.tolist())\n",
    "\n",
    "df_train = df_train[cols_keep + [predict_col]]\n",
    "df_test = df_test[cols_keep]\n",
    "\n",
    "print(\"Las columnas tras eliminar irrelevantes son:\", df_train.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd1ab29-cbca-4bd9-87b3-6e6fb4a97f4b",
   "metadata": {},
   "source": [
    "## Manejo de datos faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e98a1cc9-a9f1-499c-b3ab-99470a0e393c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sex           0\n",
       "Pclass        0\n",
       "Age         177\n",
       "SibSp         0\n",
       "Parch         0\n",
       "Embarked      2\n",
       "Cabin       687\n",
       "Fare          0\n",
       "Survived      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15742134-01c4-4e3f-bb33-eec9d4deff1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sex           0\n",
       "Pclass        0\n",
       "Age          86\n",
       "SibSp         0\n",
       "Parch         0\n",
       "Embarked      0\n",
       "Cabin       327\n",
       "Fare          1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab0abfa5-e267-4063-af23-7b4ddc823169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos función para imputación simple\n",
    "def simple_imputation(col_name, training, test, strategy='median'):\n",
    "    \"\"\"\n",
    "    Realiza una imputación simple de valores faltantes en una columna específica de los \n",
    "    conjuntos de entrenamiento y test, utilizando la estrategia especificada.\n",
    "\n",
    "    Args:\n",
    "        strategy (str, opcional): La estrategia de imputación a utilizar. Puede ser 'mean', \n",
    "                                  'median', 'most_frequent', o 'constant'. Por defecto es 'median'.\n",
    "        col_name (str): El nombre de la columna sobre la cual se realizará la imputación.\n",
    "        training (pd.DataFrame): El conjunto de datos de entrenamiento que contiene la columna con valores faltantes.\n",
    "        test (pd.DataFrame): El conjunto de datos de prueba que contiene la columna con valores faltantes.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Una tupla que contiene los conjuntos de datos de entrenamiento y prueba después de aplicar la imputación.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Si el nombre de la columna no existe en los DataFrames proporcionados.\n",
    "    \"\"\"\n",
    "\n",
    "    # Definición del imputador\n",
    "    imputer = SimpleImputer(strategy=strategy)\n",
    "\n",
    "    # Entrenamiento del imputer + transformación en training\n",
    "    training[col_name] = imputer.fit_transform(training[[col_name]]).ravel()  # ravel para aplanar la matriz 2D\n",
    "\n",
    "    # Transformación en test\n",
    "    test[col_name] = imputer.transform(test[[col_name]]).ravel()  # ravel para aplanar la matriz 2D\n",
    "\n",
    "    return training, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "467c3213-4aa2-4cb2-bb48-cfa709518a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputamos en columna 'Age' con la mediana\n",
    "df_train, df_test = simple_imputation('Age', df_train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1d8325e-5c03-405e-917f-72a1b591ff9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con cabin, puesto que creamos variable auxiliar de 'Has_Cabin', lo hacemos aquí y eliminamos la original\n",
    "df_train['Has_Cabin'] = df_train['Cabin'].notnull().astype(int)\n",
    "df_train = df_train.drop('Cabin', axis=1)\n",
    "df_test['Has_Cabin'] = df_test['Cabin'].notnull().astype(int)\n",
    "df_test = df_test.drop('Cabin', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc14bb54-34ac-4522-928f-d7bb881e84c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputamos en columna 'Embarked' con la moda (categórica)\n",
    "df_train, df_test = simple_imputation('Embarked', df_train, df_test, 'most_frequent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "396fc0e7-8e1a-4395-ab4f-dfe72f4f9312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Al detectar que tenemos un missing value en test para 'Fare', lo imputamos con la mediana de training\n",
    "_, df_test = simple_imputation('Fare', df_train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e707513-a183-40c5-a3dd-7d53ffc26f66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sex          0\n",
       "Pclass       0\n",
       "Age          0\n",
       "SibSp        0\n",
       "Parch        0\n",
       "Embarked     0\n",
       "Fare         0\n",
       "Survived     0\n",
       "Has_Cabin    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check para ver si hemos imputado correctamente en train\n",
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f08bca61-205c-4bf5-b058-fbd7959fa8b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sex          0\n",
       "Pclass       0\n",
       "Age          0\n",
       "SibSp        0\n",
       "Parch        0\n",
       "Embarked     0\n",
       "Fare         0\n",
       "Has_Cabin    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check para ver si hemos imputado correctamente en test\n",
    "df_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07fe30d-18d9-46ae-9ef4-ba82dce674ff",
   "metadata": {},
   "source": [
    "## Manejo de outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2d4a1c-ae26-41eb-bc6a-f189edf60220",
   "metadata": {},
   "source": [
    "En este caso hemos estudiado los outliers y parece que son desviaciones estadísticas, nada que necesitemos eliminar o imputar para nuestro problema."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a471ecbc-8f61-4e73-939c-25137d92481d",
   "metadata": {},
   "source": [
    "## Gestión de tipos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a71a9d2-2b0e-4066-8889-b289e5383b30",
   "metadata": {},
   "source": [
    "El único tipo que teníamos que cambiar era el de Age a integer una vez imputásemos los missing values, pero vamos a crear la columna 'Age_bin' y eliminar esta, por lo que no necesitamos hacerlo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a6410e-14b4-4e43-95ba-8f47e78f4ca4",
   "metadata": {},
   "source": [
    "## Codificación de variables categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7445399-226d-48f1-af34-e6c9b2716545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_column(col_name, training, test):\n",
    "    \"\"\"\n",
    "    Aplica One-Hot Encoding a una columna categórica en los DataFrames de entrenamiento y prueba.\n",
    "    \n",
    "    La función codifica la columna indicada en ambos DataFrames utilizando One-Hot Encoding\n",
    "    con el encoder de scikit-learn. Asegura que ambos DataFrames tengan las mismas columnas \n",
    "    después de la codificación. Además, elimina una columna codificada para evitar multicolinealidad,\n",
    "    y convierte los valores resultantes a enteros.\n",
    "\n",
    "    Args:\n",
    "        col_name (str): El nombre de la columna categórica a codificar.\n",
    "        training (pd.DataFrame): El DataFrame de entrenamiento.\n",
    "        test (pd.DataFrame): El DataFrame de prueba.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Una tupla que contiene dos DataFrames (entrenamiento y prueba) \n",
    "               con la columna especificada codificada mediante One-Hot Encoding.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Inicializar OneHotEncoder\n",
    "    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')  # Sparse=False para obtener un array de numpy; handle_unknown='ignore' para ignorar valores desconocidos\n",
    "    \n",
    "    # Ajustar el encoder con el DataFrame de entrenamiento y transformar\n",
    "    training_encoded = encoder.fit_transform(training[[col_name]])\n",
    "    test_encoded = encoder.transform(test[[col_name]])  # Usar transform para test sin ajustar de nuevo\n",
    "    \n",
    "    # Convertir los resultados codificados a DataFrames con los nombres de columnas adecuados\n",
    "    training_encoded_df = pd.DataFrame(training_encoded, \n",
    "                                       columns=encoder.get_feature_names_out([col_name]),\n",
    "                                       index=training.index)\n",
    "    \n",
    "    test_encoded_df = pd.DataFrame(test_encoded, \n",
    "                                   columns=encoder.get_feature_names_out([col_name]),\n",
    "                                   index=test.index)\n",
    "    \n",
    "    # Comprobar si todas las columnas son nulas en test (categoría desconocida)\n",
    "    if test_encoded_df.isnull().all().any():\n",
    "        print('Variable desconocida en test')\n",
    "        test_encoded_df[col_name + '_desconocidos'] = (test_encoded_df.sum(axis=1) == 0).astype(int)\n",
    "        training_encoded_df[col_name + '_desconocidos'] = 0       \n",
    "\n",
    "    # Convertir los valores codificados a enteros\n",
    "    training_encoded_df = training_encoded_df.astype(int)\n",
    "    test_encoded_df = test_encoded_df.astype(int)\n",
    "    \n",
    "    # Eliminar una columna codificada para evitar multicolinealidad (por ejemplo, la primera columna)\n",
    "    training_encoded_df = training_encoded_df.drop(columns=encoder.get_feature_names_out([col_name])[0])\n",
    "    test_encoded_df = test_encoded_df.drop(columns=encoder.get_feature_names_out([col_name])[0])\n",
    "    \n",
    "    # Combinar los DataFrames codificados con los DataFrames originales, excluyendo la columna original\n",
    "    training_combined = pd.concat([training.drop(columns=[col_name]), training_encoded_df], axis=1)\n",
    "    test_combined = pd.concat([test.drop(columns=[col_name]), test_encoded_df], axis=1)\n",
    "    \n",
    "    # Retornar los DataFrames de entrenamiento y prueba codificados\n",
    "    return training_combined, test_combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05185e35-1556-4552-bc55-9acf845f47dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codificación de 'Sex' (one-hot)\n",
    "df_train, df_test = one_hot_encode_column('Sex', df_train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6a92ced-3015-4372-9eaf-f62008ed3a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codificación de 'Embarked' (one-hot)\n",
    "df_train, df_test = one_hot_encode_column('Embarked', df_train, df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6924c00-58cb-40c5-95d8-a6563fc477ff",
   "metadata": {},
   "source": [
    "## Normalización y estandarización"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38f3803-f0f6-425d-972a-7a088bc4e8cb",
   "metadata": {},
   "source": [
    "Hemos transformado todas las variables numéricas en categorías, por lo que no hay que normalizar/estandarizar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4802829e-2569-4616-b411-f4cc6e68d9c6",
   "metadata": {},
   "source": [
    "## Transformaciones de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee4a98dd-4a37-43fc-9c20-a758b9c88011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformación de Age -> Age_bin (cambio ['Child', 'Teenager', 'Adult', 'Elderly'] -> [0, 1, 2, 3] para tenerlo codificado)\n",
    "df_train['Age_bin'] = pd.cut(df_train['Age'], bins=[0, 12, 18, 60, 80], labels=[0, 1, 2, 3])\n",
    "df_train = df_train.drop('Age', axis=1)\n",
    "df_test['Age_bin'] = pd.cut(df_test['Age'], bins=[0, 12, 18, 60, 80], labels=[0, 1, 2, 3])\n",
    "df_test = df_test.drop('Age', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01e9e6d6-b6e9-4c23-8bd4-38c961a399f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación de 'FamilySize'\n",
    "df_train['FamilySize'] = df_train['SibSp'] + df_train['Parch'] + 1\n",
    "df_train = df_train.drop(columns=['SibSp', 'Parch'], axis=1)\n",
    "df_test['FamilySize'] = df_test['SibSp'] + df_test['Parch'] + 1\n",
    "df_test = df_test.drop(columns=['SibSp', 'Parch'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74787464-db03-4129-bcd6-a8d4c0c5eeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación de 'Fare_Range' y codificación\n",
    "\n",
    "# Calcular los cuartiles en df_train y recuperar los límites\n",
    "_, bins = pd.qcut(df_train['Fare'], 4, retbins=True)  # retbins=True para obtener los límites de los cuartiles generados\n",
    "df_train['Fare_Range'] = pd.cut(df_train['Fare'], bins=bins)\n",
    "df_train = df_train.drop('Fare', axis=1)\n",
    "\n",
    "# Aplicar los mismos límites a df_test usando pd.cut\n",
    "df_test['Fare_Range'] = pd.cut(df_test['Fare'], bins=bins)\n",
    "df_test = df_test.drop('Fare', axis=1)\n",
    "\n",
    "# Codificación de la nueva variable\n",
    "fare_encoder = LabelEncoder()\n",
    "df_train['Fare_Range'] = fare_encoder.fit_transform(df_train['Fare_Range'])\n",
    "df_test['Fare_Range'] = fare_encoder.transform(df_test['Fare_Range'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c5b3739-60c9-4e6f-8f54-c76e7f41cfcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Has_Cabin</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>Age_bin</th>\n",
       "      <th>FamilySize</th>\n",
       "      <th>Fare_Range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pclass  Survived  Has_Cabin  Sex_male  Embarked_Q  Embarked_S Age_bin  \\\n",
       "0         3         0          0         1           0           1       2   \n",
       "1         1         1          1         0           0           0       2   \n",
       "2         3         1          0         0           0           1       2   \n",
       "3         1         1          1         0           0           1       2   \n",
       "4         3         0          0         1           0           1       2   \n",
       "..      ...       ...        ...       ...         ...         ...     ...   \n",
       "886       2         0          0         1           0           1       2   \n",
       "887       1         1          1         0           0           1       2   \n",
       "888       3         0          0         0           0           1       2   \n",
       "889       1         1          1         1           0           0       2   \n",
       "890       3         0          0         1           1           0       2   \n",
       "\n",
       "     FamilySize  Fare_Range  \n",
       "0             2           0  \n",
       "1             2           3  \n",
       "2             1           1  \n",
       "3             2           3  \n",
       "4             1           1  \n",
       "..          ...         ...  \n",
       "886           1           1  \n",
       "887           1           2  \n",
       "888           4           2  \n",
       "889           1           2  \n",
       "890           1           0  \n",
       "\n",
       "[891 rows x 9 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32ca8ebe-d721-470f-a855-e1242f525bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Has_Cabin</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>Age_bin</th>\n",
       "      <th>FamilySize</th>\n",
       "      <th>Fare_Range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pclass  Has_Cabin  Sex_male  Embarked_Q  Embarked_S Age_bin  FamilySize  \\\n",
       "0         3          0         1           1           0       2           1   \n",
       "1         3          0         0           0           1       2           2   \n",
       "2         2          0         1           1           0       3           1   \n",
       "3         3          0         1           0           1       2           1   \n",
       "4         3          0         0           0           1       2           3   \n",
       "..      ...        ...       ...         ...         ...     ...         ...   \n",
       "413       3          0         1           0           1       2           1   \n",
       "414       1          1         0           0           0       2           1   \n",
       "415       3          0         1           0           1       2           1   \n",
       "416       3          0         1           0           1       2           1   \n",
       "417       3          0         1           0           0       2           3   \n",
       "\n",
       "     Fare_Range  \n",
       "0             0  \n",
       "1             0  \n",
       "2             1  \n",
       "3             1  \n",
       "4             1  \n",
       "..          ...  \n",
       "413           1  \n",
       "414           3  \n",
       "415           0  \n",
       "416           1  \n",
       "417           2  \n",
       "\n",
       "[418 rows x 8 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6792f0d-296e-4c73-9aeb-c5404e82f1e9",
   "metadata": {},
   "source": [
    "## Escritura de los dataframes resultantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3d52159-a537-4373-85c8-1becf613e03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carpeta ./00_Data/Cleaned/ eliminada.\n",
      "DataFrames guardados en ./00_Data/Cleaned/:\n",
      " - train_clean.csv\n",
      " - test_clean.csv\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_FOLDER = \"./00_Data/Cleaned/\"\n",
    "\n",
    "def save_dataframes_to_csv(output_folder, df_train, df_test, train_filename=\"train_clean.csv\", test_filename=\"test_clean.csv\"):\n",
    "    \"\"\"\n",
    "    Guarda los DataFrames de entrenamiento y prueba en formato CSV en una carpeta específica.\n",
    "    Si la carpeta ya existe, borra todo su contenido antes de guardar los nuevos archivos.\n",
    "    \n",
    "    Args:\n",
    "        output_folder (str): La ruta de la carpeta donde se guardarán los archivos CSV.\n",
    "        df_train (pd.DataFrame): El DataFrame de entrenamiento que se va a guardar.\n",
    "        df_test (pd.DataFrame): El DataFrame de prueba que se va a guardar.\n",
    "        train_filename (str, opcional): El nombre del archivo CSV para el DataFrame de entrenamiento.\n",
    "        test_filename (str, opcional): El nombre del archivo CSV para el DataFrame de prueba.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Si la carpeta ya existe, eliminar todo su contenido\n",
    "    if os.path.exists(output_folder):\n",
    "        shutil.rmtree(output_folder)  # Borrar toda la carpeta y su contenido\n",
    "        print(f\"Carpeta {output_folder} eliminada.\")\n",
    "    \n",
    "    # Crear la carpeta si no existe\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Definir las rutas completas de los archivos\n",
    "    train_path = os.path.join(output_folder, train_filename)\n",
    "    test_path = os.path.join(output_folder, test_filename)\n",
    "    \n",
    "    # Guardar los DataFrames en formato CSV\n",
    "    df_train.to_csv(train_path, index=False)\n",
    "    df_test.to_csv(test_path, index=False)\n",
    "    \n",
    "    print(f\"DataFrames guardados en {output_folder}:\")\n",
    "    print(f\" - {train_filename}\")\n",
    "    print(f\" - {test_filename}\")\n",
    "\n",
    "save_dataframes_to_csv(OUTPUT_FOLDER, df_train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f1d53a-e749-4735-9b9b-460f72855c3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proyecto_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
